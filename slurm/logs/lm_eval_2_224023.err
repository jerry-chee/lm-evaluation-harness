No config specified, defaulting to: lambada_openai/default
Found cached dataset lambada_openai (/home/jc3464/.cache/huggingface/datasets/EleutherAI___lambada_openai/default/1.0.0/57baddecfa09d1790541ef07274c5666abfbe9d2ccd0cd46013cd557b0343095)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 378.48it/s]
0it [00:00, ?it/s]0it [00:00, ?it/s]
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<01:39,  1.00s/it]  3%|▎         | 3/100 [00:01<00:33,  2.93it/s] 27%|██▋       | 27/100 [00:01<00:02, 32.06it/s] 68%|██████▊   | 68/100 [00:01<00:00, 88.10it/s] 88%|████████▊ | 88/100 [00:01<00:00, 92.44it/s]100%|██████████| 100/100 [00:01<00:00, 58.47it/s]
Found cached dataset hellaswag (/home/jc3464/.cache/huggingface/datasets/hellaswag/default/0.1.0/c37cd37196278995f42bc32f532730ae9b0d5f0f4a2d3b97735c17ff3ad67169)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 230.49it/s]
  0%|          | 0/37090 [00:00<?, ?it/s]  0%|          | 0/37090 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/main.py", line 119, in <module>
    main()
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/main.py", line 86, in main
    results = evaluator.simple_evaluate(
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/utils.py", line 161, in _wrapper
    return fn(*args, **kwargs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/evaluator.py", line 98, in simple_evaluate
    results = evaluate(
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/utils.py", line 161, in _wrapper
    return fn(*args, **kwargs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/evaluator.py", line 259, in evaluate
    resps = getattr(lm, reqtype)([req.args for req in reqs])
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 820, in fn
    rem_res = getattr(self.lm, attr)(remaining_reqs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 185, in loglikelihood
    return self._loglikelihood_tokens(new_reqs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 295, in _loglikelihood_tokens
    self._model_call(batched_inps), dim=-1
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/models/gpt2.py", line 152, in _model_call
    return self.gpt2(inps)[0][:, :, :50257]
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/accelerate/hooks.py", line 156, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/jc3464/QuantHerd/quant-balance-herd-smooth/quantize/fake_quant.py", line 291, in forward
    return self.model(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 934, in forward
    outputs = self.model.decoder(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 698, in forward
    layer_outputs = decoder_layer(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 327, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 209, in forward
    attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat2 in method wrapper_bmm)
Found cached dataset piqa (/home/jc3464/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  5.25it/s] 67%|██████▋   | 2/3 [00:00<00:00,  5.01it/s]100%|██████████| 3/3 [00:00<00:00,  6.99it/s]
  0%|          | 0/3676 [00:00<?, ?it/s]  0%|          | 0/3676 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/main.py", line 119, in <module>
    main()
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/main.py", line 86, in main
    results = evaluator.simple_evaluate(
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/utils.py", line 161, in _wrapper
    return fn(*args, **kwargs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/evaluator.py", line 98, in simple_evaluate
    results = evaluate(
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/utils.py", line 161, in _wrapper
    return fn(*args, **kwargs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/evaluator.py", line 259, in evaluate
    resps = getattr(lm, reqtype)([req.args for req in reqs])
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 820, in fn
    rem_res = getattr(self.lm, attr)(remaining_reqs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 185, in loglikelihood
    return self._loglikelihood_tokens(new_reqs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 295, in _loglikelihood_tokens
    self._model_call(batched_inps), dim=-1
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/models/gpt2.py", line 152, in _model_call
    return self.gpt2(inps)[0][:, :, :50257]
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/accelerate/hooks.py", line 156, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/jc3464/QuantHerd/quant-balance-herd-smooth/quantize/fake_quant.py", line 291, in forward
    return self.model(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 934, in forward
    outputs = self.model.decoder(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 698, in forward
    layer_outputs = decoder_layer(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 327, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 209, in forward
    attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat2 in method wrapper_bmm)
Found cached dataset winogrande (/home/jc3464/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  3.07it/s] 67%|██████▋   | 2/3 [00:00<00:00,  3.92it/s]100%|██████████| 3/3 [00:00<00:00,  5.44it/s]
  0%|          | 0/2534 [00:00<?, ?it/s]  0%|          | 0/2534 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/main.py", line 119, in <module>
    main()
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/main.py", line 86, in main
    results = evaluator.simple_evaluate(
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/utils.py", line 161, in _wrapper
    return fn(*args, **kwargs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/evaluator.py", line 98, in simple_evaluate
    results = evaluate(
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/utils.py", line 161, in _wrapper
    return fn(*args, **kwargs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/evaluator.py", line 259, in evaluate
    resps = getattr(lm, reqtype)([req.args for req in reqs])
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 820, in fn
    rem_res = getattr(self.lm, attr)(remaining_reqs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 185, in loglikelihood
    return self._loglikelihood_tokens(new_reqs)
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/base.py", line 295, in _loglikelihood_tokens
    self._model_call(batched_inps), dim=-1
  File "/home/jc3464/QuantHerd/lm-evaluation-harness/lm_eval/models/gpt2.py", line 152, in _model_call
    return self.gpt2(inps)[0][:, :, :50257]
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/accelerate/hooks.py", line 156, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/jc3464/QuantHerd/quant-balance-herd-smooth/quantize/fake_quant.py", line 291, in forward
    return self.model(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 934, in forward
    outputs = self.model.decoder(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 698, in forward
    layer_outputs = decoder_layer(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 327, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jc3464/anaconda3/envs/lm_eval/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 209, in forward
    attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat2 in method wrapper_bmm)
slurmstepd: error: *** JOB 224023 ON yu-compute-01 CANCELLED AT 2023-01-12T16:49:33 ***
