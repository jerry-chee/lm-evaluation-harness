#!/bin/bash
#SBATCH --job-name=1_vbal_lm_eval
#SBATCH -N 1
#SBATCH --cpus-per-task 16
#SBATCH --mem=40G
#SBATCH --partition=desa
#SBATCH --gres=gpu:2
#SBATCH --time 24:00:00
#SBATCH --constraint="gpu-low"
#SBATCH --output "slurm/logs/lm_eval_vb_1_%j.out"                  # Name of stdout output     log file (%j expands to jobID)
#SBATCH --error "slurm/logs/lm_eval_vb_1_%j.err"                  # Name of stderr output     log file (%j expands to jobID)

# evaluates model with lm-eval
source ~/.bashrc
source ~/anaconda3/etc/profile.d/conda.sh
conda activate lm_eval

export TMPDIR=/home/jc3464/tmpdir
mktemp -u

model="opt-6.7b"
Abit=8
WPATH=/home/jc3464/QuantHerd/quant-balance-herd-smooth/checkpoints_balancequant_fix/
for Wbit in 2 3 4
do
Qbit=8
NAME="${model}_VecBal_N16384_W${Wbit}A${Abit}Q${Qbit}_per_tensor_noZ_Bias"
echo ${NAME}
for task in "lambada_openai" "hellaswag" "piqa" "winogrande" "openbookqa" "rte" "copa" "wikitext"
do
python main.py \
    --model gpt2 \
    --model_args pretrained=${WPATH}${NAME} \
    --offload_folder lm_offload/vbal/tmp1 \
    --tasks ${task} \
    --output_path slurm/output/${NAME}_${task} \
    --custom_load \
    --custom_name "facebook/${model}" \
    --custom_a_bits ${Abit}
done
done